{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d88754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install seaborn pandas matplotlib numpy    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a46bc5",
   "metadata": {},
   "source": [
    "every successive data of each device id were in an interval of approx 60 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70699995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd \n",
    "from datetime import date , time \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from traceback import print_exc\n",
    "import datetime\n",
    "file_names = [\"m1.csv\" , \"m2.csv\" , \"m3.csv\" , \"m4.csv\"]\n",
    "save_path = \"combined_mahanagar_data.csv\" \n",
    "rcp = \"rush_clasified.csv\" #rush_classified_path\n",
    "dpd = \"data_per_day.csv\" #data_per_day\n",
    "save_path_2 = \"data_tracking.csv\"\n",
    "columns = [\"distance\" , \"totalDistance\" , \"deviceId\",\"fixTime\",\"latitude\",\"longitude\",\"speed\"]\n",
    "total_bus , total_dates = [],[]\n",
    "peak_morning = [8,11]\n",
    "peak_evening = [16,19]\n",
    "dates_given = ['2020-02-23', '2019-08-22', '2020-02-24', '2020-02-25', '2020-02-26', '2020-02-27', '2020-02-28', '2020-02-06', '2020-01-27', '2020-02-07', '2020-02-08', '2020-02-09', '2020-02-10', '2020-02-11', '2020-02-14', '2020-02-15', '2020-02-16', '2020-02-17', '2020-02-18', '2020-02-19', '2020-02-20']\n",
    "dict_dates = {\n",
    "        key:0 for key in dates_given \n",
    "        \n",
    "    }\n",
    "dict_dates_2 = {\n",
    "    key:[0,0,0] for key in dates_given\n",
    "}\n",
    "buses = [130, 2, 131, 132, 133, 134, 135, 136, 200, 137, 73, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169]\n",
    "total_buses = {}\n",
    "total_dates = []\n",
    "folder_img = \"distance_vs_time\"\n",
    "os.makedirs(folder_img, exist_ok=True)\n",
    "total_entries=[0,0,0]\n",
    "discarded_entries = [0,0,0]\n",
    "def make_int(x):\n",
    "    x=str(x)\n",
    "    \n",
    "    y=list(x)\n",
    "    \n",
    "    if \"-\" in y:\n",
    "        y = x \n",
    "        year = int(\"\".join(x[0:4]))\n",
    "        month = int(\"\".join(x[5:7]))\n",
    "        day = int(\"\".join(x[8:10]))\n",
    "        return year , month , day\n",
    "    else:\n",
    "        year = int(\"\".join(x[0:2]))\n",
    "        month = int(\"\".join(x[3:5]))\n",
    "        day = int(\"\".join(x[6:8]))\n",
    "        return year , month , day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "total_lines = 0\n",
    "total_discarded = []\n",
    "def classify_hr(hour):\n",
    "    \"\"\"Classifies a time (in absolute seconds) into morning, evening, or free period.\"\"\"\n",
    "    if peak_morning[0] <= hour <= peak_morning[1]:\n",
    "        return 0\n",
    "    if peak_evening[0] <= hour <= peak_evening[1]:\n",
    "        return 2\n",
    "    return 1\n",
    "def try_float_convert(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        \n",
    "        return value\n",
    "with open(save_path ,\"w\",newline=\"\") as file:\n",
    "    writer_file = csv.DictWriter(file , fieldnames=columns)\n",
    "    writer_file.writeheader()\n",
    "    for name in file_names:\n",
    "        with open(name ,\"r\") as csv_file: \n",
    "            reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            \n",
    "            \n",
    "            for line in reader:\n",
    "                lines_to_write = [\n",
    "                    try_float_convert(line[cols])  \n",
    "                    for cols in columns \n",
    "                    #if line[cols] not in columns and line[\"alarm\"]!=\"powerCut\"\n",
    "                    if line[cols] not in columns\n",
    "                    ]\n",
    "                try:\n",
    "                    dict_temp = {\n",
    "                    columns[0]:lines_to_write[0],\n",
    "                    columns[1]:lines_to_write[1],\n",
    "                    columns[2]:int(lines_to_write[2]),\n",
    "                    columns[3]:lines_to_write[3],\n",
    "                    columns[4]:lines_to_write[4],\n",
    "                    columns[5]:lines_to_write[5],\n",
    "                    columns[6]:lines_to_write[6],\n",
    "                }\n",
    "                    writer_file.writerow(dict_temp)\n",
    "                    dict_temp={}\n",
    "                    total_lines+=1 \n",
    "                except:\n",
    "                    pass \n",
    "    file.close()\n",
    "print(f\" a total of {total_lines} were created in {save_path}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def date_parser(dates):\n",
    "    listed = []\n",
    "    i=1\n",
    "    print(dates[:10])\n",
    "    for date in dates:\n",
    "        \n",
    "        i+=1        \n",
    "        stinged = list(date)\n",
    "        \n",
    "        year = int(float(\"\".join(stinged[0:4])))\n",
    "        month = int(float(\"\".join(stinged[5:7])))\n",
    "        day=int(float(\"\".join(stinged[8:10])))\n",
    "        time1 = time(int(float(\"\".join(stinged[11:13]))),int(float(\"\".join(stinged[14:16]))) , int(float(\"\".join(stinged[17:19]))))\n",
    "        \n",
    "        d1=datetime.date(year,month , day)\n",
    "        \n",
    "        listed.append(str(datetime.datetime.combine( d1, time1))) \n",
    "    return listed\n",
    "df = pd.read_csv(save_path,header=0)\n",
    "\n",
    "df[\"fixTime\"] = date_parser(df[\"fixTime\"])\n",
    "df.to_csv(save_path , index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497734ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "with open(save_path , \"r\") as file:\n",
    "    \n",
    "    lines = csv.DictReader(file)\n",
    "    temp_recorder = [[0,0,0] for _ in range(0 , 41)]\n",
    "\n",
    "    \n",
    "\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        device_id = int(line[\"deviceId\"])\n",
    "        indexed = buses.index(device_id)\n",
    "        \n",
    "        time1 = line[\"fixTime\"]\n",
    "        time1 = time1.split() \n",
    "        yr , mon ,day = make_int(time1[0]) \n",
    "        hr,min,sec =  make_int(time1[1]) \n",
    "        # if time1[0] not in dates_given:\n",
    "        #     dates_given.append(time1[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #morning_peak 0 , free_hour 1 , evening_peak 2\n",
    "        hour_time = 0 if peak_morning[0]<=hr<=peak_morning[1] else (1 if peak_morning[1]<hr<peak_evening[0] else (2 if peak_evening[0]<=hr<=peak_evening[1] else 1))\n",
    "        total_entries[hour_time]+=1 \n",
    "        temp_recorder[indexed][hour_time]+=1\n",
    "        dict_dates[time1[0]]+=1\n",
    "        some_data = dict_dates_2[time1[0]]\n",
    "        some_data[hour_time]+=1 \n",
    "        dict_dates_2[time1[0]] = some_data\n",
    "    file.close()\n",
    "with open(rcp , \"w\") as rcp_writer:\n",
    "    writer = csv.DictWriter(rcp_writer , fieldnames=[\"device_id\" , \"morning_peak(7-10)\" , \"free_hour\" , \"evening_peak(4-7)\"])\n",
    "    writer.writeheader()\n",
    "    for i in range(0,41):\n",
    "        writer.writerow({\n",
    "            \"device_id\":buses[i] , \n",
    "            \"morning_peak(7-10)\":temp_recorder[i][0],\n",
    "            \"free_hour\":temp_recorder[i][1],\n",
    "            \"evening_peak(4-7)\":temp_recorder[i][2]\n",
    "            })      \n",
    "    rcp_writer.close()\n",
    "with open(dpd , \"w\") as dpd_writer:\n",
    "    writer = csv.DictWriter(dpd_writer , fieldnames = [\"date\" ,\"data_count\" , \"morning_rush\" , \"free\" , \"evening_rush\"])\n",
    "    writer.writeheader()\n",
    "    for key,value in dict_dates.items():\n",
    "        extra_val = dict_dates_2[key]\n",
    "        temp_dict = {\n",
    "            \"date\":key,\n",
    "            \"data_count\":value,\n",
    "            \"morning_rush\":extra_val[0],\n",
    "            \"free\":extra_val[1],\n",
    "            \"evening_rush\":extra_val[2]\n",
    "        } \n",
    "        writer.writerow(temp_dict)\n",
    "    dpd_writer.close()\n",
    "            \n",
    "    # for key,value in total_buses.iter():\n",
    "    #     print(f\"Bus {key} has {value} numbers of log in the dataset \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ee86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categories and values\n",
    "df = pd.read_csv(rcp)\n",
    "categories = df['device_id']\n",
    "group_a_values = df['morning_peak(7-10)']\n",
    "group_b_values = df['free_hour']\n",
    "group_c_values = df['evening_peak(4-7)']\n",
    "\n",
    "# Set bar width and positions\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(categories)) # Numerical positions for categories\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "# Plot each group of bars\n",
    "bar1 = ax.bar(index - bar_width, group_a_values, bar_width, label='morning_peak(7-10)')\n",
    "bar2 = ax.bar(index, group_b_values, bar_width, label='free_hour')\n",
    "bar3 = ax.bar(index + bar_width, group_c_values, bar_width, label='evening_peak(4-7)')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Device_id')\n",
    "ax.set_ylabel('No of datas')\n",
    "ax.set_title('Mahanagar dataset 1.0')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categories and values\n",
    "df = pd.read_csv(dpd)\n",
    "categories = df['date']\n",
    "\n",
    "group_a_values = df['morning_rush']\n",
    "group_b_values = df['free']\n",
    "group_c_values = df['evening_rush']\n",
    "group_d_values = df[\"data_count\"]\n",
    "# Set bar width and positions\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(categories)) # Numerical positions for categories\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(26, 6))\n",
    "\n",
    "# Plot each group of bars\n",
    "bar1 = ax.bar(index - bar_width, group_a_values, bar_width, label='morning_peak')\n",
    "bar2 = ax.bar(index, group_b_values, bar_width, label='free_hour')\n",
    "bar3 = ax.bar(index + bar_width, group_c_values, bar_width, label='evening_peak')\n",
    "bar4 = ax.bar(index + bar_width*1.5, group_d_values, bar_width, label=\"total count\") \n",
    "\n",
    "ax.set_xlabel('dates')\n",
    "ax.set_ylabel(\"data_counts\")\n",
    "ax.set_title('Mahanagar dataset 1.1')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_info = {\n",
    "    key:[] for key in buses\n",
    "}\n",
    "import math\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Returns the Haversine distance between two lat/lon points in meters.\n",
    "    \"\"\"\n",
    "    R = 6371000  # Earth radius in meters\n",
    "\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(dphi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "    \n",
    "with open(save_path , \"r\") as file:\n",
    "    \n",
    "    take_date = dates_given[4]\n",
    "    exp_yr , exp_mon,exp_day = make_int(take_date)\n",
    "    print(exp_yr)\n",
    "    reader = csv.DictReader(file)\n",
    "    for line in reader:\n",
    "        \n",
    "        date_choose = line[\"fixTime\"]\n",
    "        date_time = date_choose.split()\n",
    "        \n",
    "        yr , mon,day = make_int(date_time[0])\n",
    "        curr_hr ,curr_min , curr_sec = 0,0,0\n",
    "        if yr == exp_yr and exp_mon == mon and exp_day == day:\n",
    "            hr,min,sec = make_int(date_time[1])\n",
    "            if hr==curr_hr and min==curr_min and curr_sec == sec:\n",
    "                v = classify_hr(hr)\n",
    "                discarded_entries[v]+=1\n",
    "                continue\n",
    "                \n",
    "            #[(hr,min,sec),lat,lon]\n",
    "            try:\n",
    "                x = dict_info[int(line[\"deviceId\"])][-1] \n",
    "                lat , lon = float(line[\"latitude\"]) , float(line[\"longitude\"])\n",
    "            except:\n",
    "                lat , lon = float(line[\"latitude\"]) , float(line[\"longitude\"])\n",
    "            \n",
    "                dict_info[int(line[\"deviceId\"])].append([(hr,min,sec) , lat,lon])\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            \n",
    "            #calc. diffn in time \n",
    "            # time_taken = (abs(hr-prev_hr))*3600 + (abs(prev_min-min))*60 + (abs(sec-prev_sec))\n",
    "            # dist = haversine_distance(lat,lon , prev_lat,prev_lon)\n",
    "            dict_info[int(line[\"deviceId\"])].append([(hr,min,sec) , lat,lon])\n",
    "            curr_hr , curr_min,curr_sec = hr,min,sec\n",
    "    file.close()     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = {\n",
    "    key:[[]] for key in buses\n",
    "}\n",
    "sorting_list = []\n",
    "fixed_point = [27.678786911652914, 85.3494674406196] #koteshwor\n",
    "def time_to_seconds(h, m, s):\n",
    "    return h*3600 + m*60 + s\n",
    "\n",
    "\n",
    "for key,value in dict_info.items():\n",
    "    print(key)\n",
    "    if value ==[]:\n",
    "        continue\n",
    "    sorted_values = sorted(value , key=lambda x:x[0] , reverse = False)\n",
    "    prev_hr,prev_min ,prev_sec = 0,0,0\n",
    "    prev_lat , prev_lon = 0,0\n",
    "    total_distance_covered=0\n",
    "    temp_list = [] \n",
    "    dist_counter = 0\n",
    "    for x in sorted_values:\n",
    "        print(x[0])\n",
    "        hr ,min,sec=x[0]\n",
    "        #print(prev_hr,prev_min , prev_sec , prev_lat,prev_lon)\n",
    "        lat,lon = x[1],x[2]\n",
    "        #print(key , hr,min,sec,lat,lon)\n",
    "        \n",
    "        if ((prev_hr ==0) and (prev_min==0) and (prev_sec==0)):\n",
    "            prev_hr , prev_min,prev_sec = x[0]\n",
    "            prev_lat , prev_lon = x[1],x[2]\n",
    "            continue \n",
    "            #calc. diffn in time\n",
    "        \n",
    "        t1 = time_to_seconds(prev_hr, prev_min, prev_sec)\n",
    "        t2 = time_to_seconds(hr, min, sec)\n",
    "\n",
    "        time_taken = t2 - t1\n",
    "        if time_taken < 10:\n",
    "            v = classify_hr(hr)\n",
    "            discarded_entries[v]+=1\n",
    "            continue\n",
    "        dist = haversine_distance(lat,lon , prev_lat,prev_lon)\n",
    "        if dist < 100:# checks if the distance covered in 60 sec is less than 10 m and\n",
    "            dist_counter+=1\n",
    "            if dist_counter > 20:# removes last 20 entries ie data of 120 min if the bus is stationary\n",
    "                v = classify_hr(hr)\n",
    "                discarded_entries[v]+=20\n",
    "                temp_list = temp_list[:-20]\n",
    "                sorting_list = sorting_list[:-20]\n",
    "        else :\n",
    "            dist_counter = 0\n",
    "        total_distance_covered +=dist\n",
    "\n",
    "        fixed_dist = haversine_distance(lat,lon , fixed_point[0],fixed_point[1])\n",
    "        sorting_list.append([(hr,min,sec) , lat,lon,time_taken , dist,total_distance_covered,fixed_dist])\n",
    "        prev_hr , prev_min,prev_sec = x[0]\n",
    "        prev_lat , prev_lon = x[1],x[2]\n",
    "        if time_taken > 600:\n",
    "            import random\n",
    "            random_num = random.randint(000 , 1000000)\n",
    "            with open(f\"round_{key}_{random_num}.csv\",\"w\") as log_file:\n",
    "                writer = csv.DictWriter(log_file , fieldnames=[\"time\",\"lat\",\"lon\",\"time_taken\",\"distance\",\"total_distance\",\"from_koteshwor\"])\n",
    "                writer.writeheader()\n",
    "                for val in temp_list:\n",
    "                    if val ==[]:\n",
    "                        continue\n",
    "                    dict_to_write = {\n",
    "                        \"time\":val[0],\n",
    "                        \"lat\":val[1],\n",
    "                        \"lon\":val[2],\n",
    "                        \"time_taken\":val[3],\n",
    "                        \"distance\":val[4],\n",
    "                        \"total_distance\":val[5],\n",
    "                        \"from_koteshwor\":val[6]\n",
    "                    }\n",
    "                    writer.writerow(dict_to_write)\n",
    "            log_file.close()\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append([(hr,min,sec) , lat,lon,time_taken , dist,total_distance_covered,fixed_dist])\n",
    "    #sorting_list = sorted(sorting_list,key = lambda x:x[0] , reverse = False)\n",
    "    sorted_dict[key] = sorting_list \n",
    "    print(sorted_dict[key])\n",
    "    sorting_list = []\n",
    "with open(save_path_2,\"w\") as file: \n",
    "    writer = csv.DictWriter(file , fieldnames=[\"deviceId\",\"time\",\"lat\",\"lon\",\"time_taken\",\"distance\",\"total_distance\",\"from_koteshwor\"])\n",
    "    writer.writeheader()\n",
    "    for key,value in sorted_dict.items():\n",
    "        for val in value:\n",
    "            if val ==[] or val[3]==0:\n",
    "                continue\n",
    "            print(\"val is :\" , val[0])\n",
    "            dict_to_write = {\n",
    "            \"deviceId\":key,\n",
    "            \"time\":val[0],\n",
    "            \"lat\":val[1],\n",
    "            \"lon\":val[2],\n",
    "            \"time_taken\":val[3],\n",
    "            \"distance\":val[4],\n",
    "            \"total_distance\":val[5],\n",
    "            \"from_koteshwor\":val[6]\n",
    "            }\n",
    "            writer.writerow(dict_to_write)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast   # <-- To safely convert \"(15, 18, 29)\" into a real tuple\n",
    "\n",
    "dict_ploting = {key: [[], []] for key in buses}\n",
    "\n",
    "with open(save_path_2, \"r\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for line in reader:\n",
    "        dev_id = int(line[\"deviceId\"])\n",
    "\n",
    "        # Convert string \"(15, 18, 29)\" into real tuple\n",
    "        print(line[\"time\"])\n",
    "        t = ast.literal_eval(line[\"time\"])\n",
    "        print(t)\n",
    "        dict_ploting[dev_id][0].append(t)\n",
    "\n",
    "        # Total distance\n",
    "        dist = float(line[\"total_distance\"])\n",
    "        dict_ploting[dev_id][1].append(dist)\n",
    "\n",
    "# ---- PLOT ----\n",
    "for bus_id, (time_list, distance_list) in dict_ploting.items():\n",
    "\n",
    "    x = list(range(len(time_list)))   # equal interval x-axis\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(x, distance_list, marker='o')\n",
    "\n",
    "    plt.title(f\"Distance vs Time for Bus {bus_id}\")\n",
    "    plt.xlabel(\"Equal Interval Points\")\n",
    "    plt.ylabel(\"Distance Covered\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Convert tuple to a readable label\n",
    "    \n",
    "    time_labels = [f\"{h:02d}:{m:02d}:{s:02d}\" for h, m, s in time_list]\n",
    "\n",
    "    # Show max 20 x-tick labels to avoid clutter\n",
    "    step = max(1, len(time_labels) // 20)\n",
    "    plt.xticks(x[::step], time_labels[::step], rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(folder_img, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Reference longitude (you can adjust slightly — this is around Baneshwor / Old Baneshwor area)\n",
    "fixed_lon = 85.32297540237306\n",
    "\n",
    "# Find all round_*.csv files\n",
    "round_files = glob.glob(\"round_*.csv\")\n",
    "\n",
    "print(f\"Found {len(round_files)} round files. Processing...\\n\")\n",
    "\n",
    "for file in round_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # We only need at least 60 rows now\n",
    "        if len(df) < 60:\n",
    "            print(f\"Skipping {file}: fewer than 60 rows (has {len(df)})\")\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # Use 50th row (index 49) and 60th row (index 59)\n",
    "        row50 = df.iloc[49]\n",
    "        row60 = df.iloc[59]\n",
    "\n",
    "        lat50, lon50 = row50['lat'], row50['lon']\n",
    "        lat60, lon60 = row60['lat'], row60['lon']\n",
    "\n",
    "        # Is the bus going north (increasing latitude)?\n",
    "        going_north = lat60 > lat50\n",
    "\n",
    "        # Which side of the reference longitude?\n",
    "        side50 = \"left\"  if lon50 < fixed_lon else \"right\"\n",
    "        side60 = \"left\"  if lon60 < fixed_lon else \"right\"\n",
    "\n",
    "        # If it crossed the reference line between 50th and 60th → too ambiguous this early\n",
    "        if side50 != side60:\n",
    "            print(f\"{file}: Crossed reference longitude between 50th and 60th row → skipping (ambiguous early segment)\")\n",
    "            continue\n",
    "\n",
    "        # Core direction logic (Kathmandu Ring Road - view from above)\n",
    "        if side50 == \"left\":   # Western half\n",
    "            direction = \"clockwise\" if going_north else \"counter_clockwise\"\n",
    "        else:                  # Eastern half\n",
    "            direction = \"counter_clockwise\" if going_north else \"clockwise\"\n",
    "\n",
    "        # Choose prefix\n",
    "        prefix = \"counter_clockwise_\" if direction == \"counter_clockwise\" else \"clockwise_\"\n",
    "\n",
    "        # Build new filename\n",
    "        new_filename = prefix + os.path.basename(file)\n",
    "        new_filepath = os.path.join(os.path.dirname(file) or '.', new_filename)\n",
    "\n",
    "        # Avoid name collision\n",
    "        if os.path.exists(new_filepath):\n",
    "            base, ext = os.path.splitext(new_filename)\n",
    "            counter = 1\n",
    "            while os.path.exists(new_filepath):\n",
    "                new_filepath = os.path.join(os.path.dirname(file) or '.', f\"{base}_{counter}{ext}\")\n",
    "                counter += 1\n",
    "\n",
    "        # Rename (move) the file\n",
    "        shutil.move(file, new_filepath)\n",
    "\n",
    "        print(f\"Success: {os.path.basename(file)}\")\n",
    "        print(f\"     → {os.path.basename(new_filepath)}\")\n",
    "        print(f\"     Side: {side50} | 60th is {'NORTH' if going_north else 'SOUTH'} of 50th → {direction.upper()}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_STOPS_ORDER = [\n",
    "    \"koteshwor\",\"airport\", \"gausala\" , \"chabhil\" , \"dhumbarahi\" , \"maharajgunj\", \"gangabu\",\n",
    "    \"samakhushi\" , \"balaju\" ,\"banasthali\", \"swoyambhu\", \"sitapaila\" ,\"balkhu\", \"ekantakuna\",\"satdobato\", \"gwarko\",\"balkumari\"\n",
    "]\n",
    "\n",
    "# Bus stations bounding boxes\n",
    "bus_stations = {\n",
    "    \"anti-clockwise\": {\n",
    "        \"koteshwor\":[27.679261426232177, 85.34936846935956 , 27.680728503597233, 85.3495489426487],\n",
    "        \"airport\":[27.70078540318246, 85.3533769530794,27.701573335528412, 85.35303133213452],#\n",
    "        \"gausala\":[27.706379161514995, 85.34493560000823,27.70725888643062, 85.34418441639279],\n",
    "        \"chabhil\":[27.717031239951016, 85.3463460556796,27.718204535772227, 85.34685091098906],\n",
    "        #\"dhumbarahi\":[27.730953680990783, 85.34441299558648,27.732020831539714, 85.34419598186516],#\n",
    "        #\"maharajgunj\":[27.73926959110684, 85.33815604032617,27.740494296415864, 85.33614465640026],#\n",
    "        \"gangabu\":[27.73756021680712, 85.32456480569198,27.738041319788685, 85.32514445895883],\n",
    "        \"samakhushi\": [27.734510272551045, 85.31315704526047,27.734988742644767, 85.31546713620789],\n",
    "        \"balaju\": [27.726016065272873, 85.3035017969009,27.728129139328153, 85.30523648797792],\n",
    "        \"banasthali\":[27.71883550527814, 85.2858452410448 , 27.719844381320847, 85.28726472663818],\n",
    "        \"swoyambhu\":[27.71540016240659, 85.28353069074375,27.716986341444347, 85.28386474803716],\n",
    "        \"sitapaila\": [27.707019479706727, 85.28256593334609,27.708527283182143, 85.28280436687375],\n",
    "        #\"balkhu\":[27.684911593611844, 85.29708170941224,27.684903832051784, 85.29964109103348],\n",
    "        #\"ekantakuna\":[27.668433982039407, 85.30668749053063,27.669623493257284, 85.30597299152524],#\n",
    "        \"satdobato\":[27.658031146589305, 85.32347613583907,27.659581479957513, 85.32579715989439],\n",
    "        \"gwarko\":[27.666433507619914, 85.33196568965492,27.667210717280035, 85.33248571687696],\n",
    "        \"balkumari\":[27.671076985194357, 85.33969391131329,27.672139477723608, 85.34068226337071]\n",
    "    },\n",
    "    \"clockwise\": {\n",
    "        \"koteshwor\":[27.679360243561582, 85.34941241032318,27.680401777864788, 85.34956815422122],\n",
    "        \"airport\":[27.700269739797584, 85.35404763465152,27.700749790561282, 85.35380200362606],\n",
    "        \"gausala\": [27.705844395215866, 85.34632228676749,27.70598189385746, 85.34821661918478],\n",
    "        \"chabhil\":[27.71673824066052, 85.34652355945491,27.717243395322154, 85.34709193532692],\n",
    "        #\"dhumbarahi\": [27.731781521213016, 85.34436394126764,27.733327061580734, 85.3435673251924],\n",
    "        #\"maharajgunj\":[27.73937459407797, 85.33839402737382,27.740490229186403, 85.33639005980756],\n",
    "        \"gangabu\":[27.73777682463683, 85.32466529203856,27.73859873484745, 85.32587780558195],\n",
    "        \"samakhushi\":[27.734851265125926, 85.31257710156471,27.735064612431227, 85.31490388005503],\n",
    "        \"balaju\": [27.72649827147075, 85.30373087544649,27.72855581557925, 85.30516384756554],\n",
    "        \"banasthali\":[27.719298335255022, 85.28578533205231,27.720177454311088, 85.2874093632639],\n",
    "        \"swoyambhu\":[27.716068321879543, 85.28346435660502,27.717078086320157, 85.28369225565842],\n",
    "        \"sitapaila\": [27.70686115913291, 85.28219976757146,27.70857251696444, 85.28269366559307],\n",
    "        #\"balkhu\":[27.68425789571755, 85.30065540395253,27.684345959864267, 85.30181369197074],\n",
    "        #\"ekantakuna\":[27.667266839181096, 85.30718816078583,27.667906124565505, 85.30703180967741],\n",
    "        \"satdobato\":[27.657961506465867, 85.3239280623327,27.659111317210236, 85.32546889945682],\n",
    "        \"gwarko\":[27.665919477021834, 85.33192390175223,27.666869403657117, 85.33249964617663],\n",
    "        \"balkumari\":[27.670955815839218, 85.33997817565354,27.672188974432935, 85.34113261637573]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ast\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import traceback\n",
    "# === Configuration and Setup ===\n",
    "\n",
    "# Folders\n",
    "FOLDER_HEATMAP_WAIT_TRAVEL = \"heatmaps_wait_travel\"\n",
    "FOLDER_HEATMAP_CUMULATIVE = \"heatmaps_cumulative\"\n",
    "FOLDER_PEAK_HEATMAPS = \"peak_heatmaps\"\n",
    "os.makedirs(FOLDER_HEATMAP_WAIT_TRAVEL, exist_ok=True)\n",
    "os.makedirs(FOLDER_HEATMAP_CUMULATIVE, exist_ok=True)\n",
    "os.makedirs(FOLDER_PEAK_HEATMAPS, exist_ok=True)\n",
    "\n",
    "# Peak hours (Ensure these variables are defined globally or passed in if running outside a script)\n",
    "\n",
    "\n",
    "PEAK_MORNING = peak_morning\n",
    "PEAK_EVENING = peak_evening\n",
    "\n",
    "# Base stop order (used for indexing heatmaps)\n",
    "BASE_STOPS_ORDER = [\n",
    "    \"koteshwor\",\"airport\", \"gausala\" , \"chabhil\" , \"gangabu\",\n",
    "    \"samakhushi\" , \"balaju\" ,\"banasthali\", \"swoyambhu\", \"sitapaila\" ,\"satdobato\", \"gwarko\",\"balkumari\"\n",
    "]\n",
    "\n",
    "\n",
    "DIRECTION_CONFIG = {\n",
    "    \"clockwise\": {\n",
    "        \"pattern\": \"clockwise_*.csv\",\n",
    "        \"dir_key\": \"clockwise\",\n",
    "        \"stops_order\": BASE_STOPS_ORDER\n",
    "    },\n",
    "    \"counter_clockwise\": {\n",
    "        \"pattern\": \"counter_clockwise_*.csv\",\n",
    "        \"dir_key\": \"anti-clockwise\",\n",
    "        \"stops_order\": list(reversed(BASE_STOPS_ORDER))\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Helper Functions (Unchanged) ===\n",
    "def time_to_seconds(t):\n",
    "    \"\"\"Converts a time tuple (h, m, s) to total seconds.\"\"\"\n",
    "    if isinstance(t, (tuple, list)) and len(t) == 3:\n",
    "        return t[0]*3600 + t[1]*60 + t[2] \n",
    "    return 0\n",
    "def time_to_hr(t):\n",
    "    if isinstance(t, (tuple, list)) and len(t) == 3:\n",
    "        return t[0]\n",
    "\n",
    "    print(\"outttside if statement\")\n",
    "    return 0\n",
    "\n",
    "def sec_to_time(sec):\n",
    "    \"\"\"Converts total seconds back to HH:MM:SS format.\"\"\"\n",
    "    sec = int(sec)\n",
    "    h = sec // 3600\n",
    "    m = (sec % 3600) // 60\n",
    "    s = sec % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    # Simple distance approximation for performance\n",
    "    return math.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "\n",
    "def get_stop(lat, lon, stations_dict, passed_stop, direction, threshold=0.003):\n",
    "    \"\"\"\n",
    "    threshold: approx 500 meters in decimal degrees\n",
    "    \"\"\"\n",
    "    # 1. First, check for an exact match (inside a box)\n",
    "    for name, (s, w, n, e) in stations_dict.items():\n",
    "        if __builtins__.min(s, n) <= lat <= max(s, n) and __builtins__.min(w, e) <= lon <= max(w, e):\n",
    "            return name\n",
    "\n",
    "    # 2. If no exact match, find the nearest stop from the expected sequence\n",
    "    if passed_stop in BASE_STOPS_ORDER:\n",
    "        curr_idx = BASE_STOPS_ORDER.index(passed_stop)\n",
    "        \n",
    "        # Check the next 2 stops in the sequence to see if we skipped one\n",
    "        for i in range(1, 3):\n",
    "            shift = i if direction == \"anti-clockwise\" else -i\n",
    "            target_idx = (curr_idx + shift) % len(BASE_STOPS_ORDER)\n",
    "            target_name = BASE_STOPS_ORDER[target_idx]\n",
    "            \n",
    "            # Get coordinates for this target stop\n",
    "            s, w, n, e = stations_dict[target_name]\n",
    "            center_lat, center_lon = (s + n) / 2, (w + e) / 2\n",
    "            \n",
    "            # Check if we are close enough to this stop\n",
    "            if get_distance(lat, lon, center_lat, center_lon) < threshold:\n",
    "                return target_name\n",
    "\n",
    "    return f\"In Transit (Passed {passed_stop})\"\n",
    "def classify_period(hr):\n",
    "    if peak_morning[0]<=hr<=peak_morning[1]:\n",
    "        return \"morning\"\n",
    "    elif peak_evening[0]<=hr<=peak_evening[1]:\n",
    "        return \"evening\"\n",
    "    return \"free\"\n",
    "\n",
    "\n",
    "# === MODIFIED HELPER FUNCTION: NEGATIVE VALUE CORRECTION ===\n",
    "\n",
    "def correct_matrix_negatives(matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the rule: if M[i, j] is negative, replace it with the absolute positive \n",
    "    value from M[j, i]. If M[j, i] is also not positive, set M[i, j] to 0.0.\n",
    "    \"\"\"\n",
    "    matrix_corrected = matrix.copy()\n",
    "    stops = matrix.index\n",
    "    \n",
    "    for i in range(len(stops)):\n",
    "        for j in range(len(stops)):\n",
    "            row_stop = stops[i]\n",
    "            col_stop = stops[j]\n",
    "            \n",
    "            value = matrix_corrected.loc[row_stop, col_stop]\n",
    "            \n",
    "            if value < 0 and i != j: # Only check off-diagonal negative values\n",
    "                # Get the reverse journey time (col_stop to row_stop)\n",
    "                matrix_corrected.loc[col_stop, row_stop]  = abs(value)\n",
    "                matrix_corrected.loc[row_stop , col_stop] = 0.0\n",
    "                \n",
    "\n",
    "    return matrix_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 1. Waiting & Travel heatmaps (per direction) - UNCHANGED ===\n",
    "# (This section generates long, non-square heatmaps which are not subject to the i/j swap logic)\n",
    "print(\"Generating Waiting and Travel Time Heatmaps (Dynamic Spectrum)...\")\n",
    "for dir_str, config in DIRECTION_CONFIG.items():\n",
    "    files = glob.glob(config[\"pattern\"])\n",
    "\n",
    "    direction = \"clockwise\" if dir_str ==\"clockwise\" else \"anti-clockwise\"\n",
    "    if not files:\n",
    "        print(f\" No files found for {dir_str}\")\n",
    "        continue\n",
    "\n",
    "    stops_order = config[\"stops_order\"]\n",
    "    stations = bus_stations[config[\"dir_key\"]]\n",
    "    \n",
    "    travel_pairs = [f\"{stops_order[i]}*to*{stops_order[(i+1) % len(stops_order)]}\" for i in range(len(stops_order))]\n",
    "    \n",
    "    waiting_data = []\n",
    "    travel_data = []\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if isinstance(df['time'].iloc[0], str) and df['time'].iloc[0].startswith('('):\n",
    "                df['time'] = df['time'].apply(ast.literal_eval)\n",
    "            \n",
    "            df['timestamp_sec'] = df['time'].apply(time_to_seconds)\n",
    "            df[\"hr\"]=df[\"time\"].apply(time_to_hr)\n",
    "            df['cum_time'] = df['time_taken'].cumsum().shift(fill_value=0)\n",
    "            stops = []\n",
    "            last_stop = None\n",
    "            for _, row in df.iterrows():\n",
    "                # def get_stop(lat, lon, stations_dict, passed_stop, direction, threshold=0.001):\n",
    "\n",
    "                current_stop = get_stop(row['lat'], row['lon'], bus_stations['anti-clockwise'], last_stop , direction = direction)\n",
    "                stops.append(current_stop)\n",
    "                last_stop = current_stop\n",
    "            \n",
    "            df['stop'] = stops\n",
    "            \n",
    "            df['group_id'] = (df['stop'] != df['stop'].shift()).cumsum()\n",
    "            groups = df.groupby('group_id')\n",
    "\n",
    "            waiting_times = {stop: 0 for stop in stops_order}\n",
    "            stop_arrivals = {}\n",
    "            stop_departures = {}\n",
    "\n",
    "            for group_id, group in groups:\n",
    "                stop = group['stop'].iloc[0]\n",
    "                if stop and pd.notna(stop) and len(group) > 1:\n",
    "                    waiting = group['cum_time'].iloc[-1] - group['cum_time'].iloc[0]\n",
    "                    waiting_times[stop] += waiting\n",
    "                    \n",
    "                    stop_arrivals[stop] = group['cum_time'].iloc[0]\n",
    "                    stop_departures[stop] = group['cum_time'].iloc[-1]\n",
    "            \n",
    "            waiting_data.append({'file': os.path.basename(file), **waiting_times})\n",
    "            \n",
    "            stop_sequence = list(dict.fromkeys([g['stop'].iloc[0] for group_id, g in groups if g['stop'].iloc[0] and pd.notna(g['stop'].iloc[0])]))\n",
    "            \n",
    "            travel_times = {pair: 0 for pair in travel_pairs}\n",
    "            for i in range(1, len(stop_sequence)):\n",
    "                from_stop = stop_sequence[i-1]\n",
    "                to_stop = stop_sequence[i]\n",
    "                key = f\"{from_stop}*to*{to_stop}\"\n",
    "                \n",
    "                if key in travel_times and from_stop in stop_departures and to_stop in stop_arrivals:\n",
    "                    travel = stop_arrivals[to_stop] - stop_departures[from_stop]\n",
    "                    if travel > 0:\n",
    "                        travel_times[key] = travel\n",
    "            \n",
    "            travel_data.append({'file': os.path.basename(file), **travel_times})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {file}: {e}\")\n",
    "\n",
    "    # Plotting Waiting Times (Now dynamic)\n",
    "    if waiting_data:\n",
    "        df_wait = pd.DataFrame(waiting_data).set_index('file').reindex(columns=stops_order).fillna(0)\n",
    "        \n",
    "        non_zero_vals = df_wait[df_wait > 0].values\n",
    "        if len(non_zero_vals) > 0:\n",
    "            wait_min = non_zero_vals.min()\n",
    "            wait_max = non_zero_vals.max()\n",
    "            mask = df_wait == 0 # Mask zero values\n",
    "        else:\n",
    "            wait_min, wait_max = 0, 1 # Fallback\n",
    "            mask = df_wait == 0\n",
    "\n",
    "        plt.figure(figsize=(12, len(files)*0.5 + 2))\n",
    "        sns.heatmap(df_wait, annot=True, fmt=\".0f\", cmap=\"YlGnBu\", \n",
    "                    cbar_kws={'label': 'Seconds'}, vmin=wait_min, vmax=wait_max, mask=mask)\n",
    "        plt.title(f\"Waiting Times at Stops - {dir_str.replace('_', ' ').title()} (Dynamic Spectrum)\")\n",
    "        plt.ylabel(\"Trip File\")\n",
    "        plt.xlabel(\"Stops\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FOLDER_HEATMAP_WAIT_TRAVEL, f\"waiting_{dir_str}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Plotting Travel Times (Now dynamic)\n",
    "    if travel_data:\n",
    "        df_travel = pd.DataFrame(travel_data).set_index('file').reindex(columns=travel_pairs).fillna(0)\n",
    "        \n",
    "        non_zero_vals = df_travel[df_travel > 0].values\n",
    "        if len(non_zero_vals) > 0:\n",
    "            travel_min = non_zero_vals.min()\n",
    "            travel_max = non_zero_vals.max()\n",
    "            mask = df_travel == 0 # Mask zero values\n",
    "        else:\n",
    "            travel_min, travel_max = 0, 1 # Fallback\n",
    "            mask = df_travel == 0\n",
    "\n",
    "        plt.figure(figsize=(len(travel_pairs)*0.8, len(files)*0.5 + 2))\n",
    "        sns.heatmap(df_travel, annot=True, fmt=\".0f\", cmap=\"OrRd\", \n",
    "                    cbar_kws={'label': 'Seconds'}, vmin=travel_min, vmax=travel_max, mask=mask)\n",
    "        plt.title(f\"Travel Times Between Stops - {dir_str.replace('_', ' ').title()} (Dynamic Spectrum)\")\n",
    "        plt.ylabel(\"Trip File\")\n",
    "        plt.xlabel(\"Route Segment\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FOLDER_HEATMAP_WAIT_TRAVEL, f\"travel_{dir_str}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# === 2. Cumulative journey heatmaps (per trip) - MODIFIED: FULL MATRIX + CORRECTION ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_files = glob.glob(\"clockwise_*.csv\") + glob.glob(\"counter_clockwise_*.csv\")\n",
    "print(f\"\\nGenerating cumulative journey heatmaps for {len(all_files)} trips (Full Matrix + Corrected)...\")\n",
    "\n",
    "for file_path in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    direction_name = \"Counter-Clockwise\" if \"counter_clockwise\" in filename else \"Clockwise\"\n",
    "    dir_key = \"anti-clockwise\" if \"counter_clockwise\" in filename else \"clockwise\"\n",
    "    route_order = BASE_STOPS_ORDER if direction_name == \"Clockwise\" else list(reversed(BASE_STOPS_ORDER))\n",
    "    stations = bus_stations[dir_key]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if isinstance(df['time'].iloc[0], str) and df['time'].iloc[0].startswith('('):\n",
    "            df['time'] = df['time'].apply(ast.literal_eval)\n",
    "        \n",
    "        df['timestamp_sec'] = df['time'].apply(time_to_seconds)\n",
    "        df[\"hr\"]=df[\"time\"].apply(time_to_hr)\n",
    "        df['cum_time'] = df['time_taken'].cumsum().fillna(0)\n",
    "        stops = []\n",
    "        last_stop = None\n",
    "        for _, row in df.iterrows():\n",
    "            current_stop = get_stop(row['lat'], row['lon'], bus_stations['anti-clockwise'], last_stop , direction = dir_str)\n",
    "            stops.append(current_stop)\n",
    "            last_stop = current_stop\n",
    "\n",
    "        df['stop'] = stops\n",
    "        \n",
    "        df['change'] = (df['stop'] != df['stop'].shift(1))\n",
    "        df['segment'] = df['change'].cumsum()\n",
    "        \n",
    "        arrival_times = {}\n",
    "        departure_times = {}\n",
    "        arrival_clocks = {}\n",
    "\n",
    "        for _, group in df.groupby('segment'):\n",
    "            stop = group['stop'].iloc[0]\n",
    "            if stop and pd.notna(stop):\n",
    "                arrival_times[stop] = group['cum_time'].iloc[0]\n",
    "                departure_times[stop] = group['cum_time'].iloc[-1]\n",
    "                arrival_clocks[stop] = group['timestamp_sec'].iloc[0]\n",
    "        \n",
    "        visited_stops = [s for s in route_order if s in arrival_times]\n",
    "\n",
    "        if len(visited_stops) < 13:\n",
    "            continue\n",
    "\n",
    "        matrix = pd.DataFrame(0.0, index=BASE_STOPS_ORDER, columns=BASE_STOPS_ORDER)\n",
    "\n",
    "        # 1. Fill Diagonal with Waiting Time (Time spent at stop)\n",
    "        for stop in visited_stops:\n",
    "            if stop in arrival_times and stop in departure_times:\n",
    "                waiting_time = (departure_times[stop] - arrival_times[stop]) / 60.0\n",
    "                matrix.loc[stop, stop] = round(waiting_time, 1)\n",
    "\n",
    "        # 2. Fill the FULL Matrix (Upper and Lower) with Cumulative Journey Time\n",
    "        # The matrix will be symmetric, showing the time from any stop 'i' to any subsequent stop 'j'.\n",
    "        for i in range(len(visited_stops)):\n",
    "            for j in range(len(visited_stops)):\n",
    "                if i == j: continue # Skip diagonal (waiting time already calculated)\n",
    "                \n",
    "                from_stop = visited_stops[i]\n",
    "                to_stop = visited_stops[j]\n",
    "                \n",
    "                # We only calculate cumulative journey time from an earlier visited stop to a later visited stop.\n",
    "                # If i < j (earlier stop to later stop):\n",
    "                \n",
    "                mins = (arrival_times[to_stop] - arrival_times[from_stop]) / 60.0\n",
    "                if mins <0.0 :\n",
    "                    continue\n",
    "                matrix.loc[from_stop, to_stop] = round(mins, 1)\n",
    "        \n",
    "        # --- APPLY NEGATIVE VALUE CORRECTION HERE ---\n",
    "        # This function will find any negative M[i,j] and try to replace it with M[j,i] if M[j,i] is positive.\n",
    "        matrix = correct_matrix_negatives(matrix)\n",
    "        \n",
    "        # --- DYNAMIC SPECTRUM AND MASKING LOGIC ---\n",
    "        valid_vals = matrix.values[matrix.values != 0.0]\n",
    "        if valid_vals.size > 0:\n",
    "            # Min/Max should still be dynamic based on the positive values after correction\n",
    "            min_val = valid_vals.min()\n",
    "            max_val = valid_vals.max()\n",
    "        else:\n",
    "            min_val, max_val = 0, 1 # Fallback\n",
    "            \n",
    "        mask = matrix == 0.0 \n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 11))\n",
    "        ax_main = plt.gca()\n",
    "        sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn_r\", linewidths=0.6, linecolor='gray',\n",
    "                    cbar_kws={'label': 'Journey Time (minutes)'}, \n",
    "                    vmin=min_val, vmax=max_val, ax=ax_main, mask=mask)\n",
    "        \n",
    "        plt.text(0.5, 1.03, \"Diagonal = Waiting Time at Stop (min) | Full Matrix Corrected for Negatives\",\n",
    "                 transform=ax_main.transAxes, fontsize=10, color='blue', ha='center')\n",
    "        \n",
    "        plt.title(f\"Cumulative Journey Time Heatmap (Full Matrix)\\n{filename}\\n{direction_name} Route (Corrected)\", fontsize=15, pad=30)\n",
    "        plt.xlabel(\"To Stop (includes Waiting Time at destination stop) →\")\n",
    "        plt.ylabel(\"From Stop (includes Waiting Time at start stop) →\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # Timeline subplot (unchanged)\n",
    "        divider = make_axes_locatable(ax_main)\n",
    "        ax_timeline = divider.append_axes(\"bottom\", size=\"25%\", pad=0.7)\n",
    "        times_sec = [arrival_clocks[s] for s in visited_stops]\n",
    "        time_labels = [sec_to_time(t) for t in times_sec]\n",
    "        \n",
    "        ax_timeline.plot(times_sec, [0]*len(times_sec), 'o', color='blue',\n",
    "                         markerfacecolor='white', markeredgewidth=2)\n",
    "        ax_timeline.set_ylim(-1, 1)\n",
    "        ax_timeline.set_yticks([])\n",
    "        for spine in ['top', 'left', 'right']:\n",
    "            ax_timeline.spines[spine].set_visible(False)\n",
    "        ax_timeline.set_xlabel(\"Absolute Arrival Time (Clock)\", fontsize=12, labelpad=15)\n",
    "        \n",
    "        for t, name in zip(times_sec, visited_stops):\n",
    "            ax_timeline.text(t, 0.35, name.capitalize().replace(\"*\", \" \"), rotation=40, ha='right', va='bottom', fontsize=10)\n",
    "        for t, label in zip(times_sec, time_labels):\n",
    "            ax_timeline.text(t, -0.5, label, rotation=40, ha='right', va='top', fontsize=9, color='gray')\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "        safe_name = os.path.splitext(filename)[0]\n",
    "        output_path = os.path.join(FOLDER_HEATMAP_CUMULATIVE, f\"cumulative_{safe_name}_full_corrected.png\")\n",
    "        plt.savefig(output_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\" Saved: cumulative_{safe_name}_full_corrected.png (Full Matrix Corrected)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\" ERROR processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09feee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3. Peak hour heatmaps (Average Journey Time + Average Waiting Time on Diagonal) - MODIFIED: FULL MATRIX + CORRECTION ===\n",
    "print(f\"\\nGenerating peak hour analysis (Average Journey & Waiting Time with full matrix correction)...\")\n",
    "peak_records = []\n",
    "\n",
    "# --- Collect Waiting Time Data (from=to) ---\n",
    "# (Unchanged)\n",
    "for dir_str, config in DIRECTION_CONFIG.items():\n",
    "    files = glob.glob(config[\"pattern\"])\n",
    "    direction = \"clockwise\" if dir_str ==\"clockwise\" else \"anti-clockwise\"\n",
    "\n",
    "    stations = bus_stations[config[\"dir_key\"]]\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if isinstance(df['time'].iloc[0], str) and df['time'].iloc[0].startswith('('):\n",
    "                df['time'] = df['time'].apply(ast.literal_eval)\n",
    "                print(df[\"time\"])\n",
    "            df['timestamp_sec'] = df['time'].apply(time_to_seconds)\n",
    "            stops = []\n",
    "            last_stop = None\n",
    "            for _, row in df.iterrows():\n",
    "                current_stop =get_stop(row['lat'], row['lon'], bus_stations['anti-clockwise'], last_stop , direction = direction)\n",
    "                stops.append(current_stop)\n",
    "                last_stop = current_stop\n",
    "\n",
    "            df['stop'] = stops\n",
    "            df[\"hr\"]=df[\"time\"].apply(time_to_hr)\n",
    "            trip_start_sec = df['timestamp_sec'].iloc[0]\n",
    "            \n",
    "\n",
    "            df['group_id'] = (df['stop'] != df['stop'].shift()).cumsum()\n",
    "            df['cum_time'] = df['time_taken'].cumsum().shift(fill_value=0)\n",
    "            groups = df.groupby('group_id')\n",
    "            \n",
    "            for group_id, group in groups:\n",
    "                stop = group['stop'].iloc[0]\n",
    "                if stop and pd.notna(stop) and len(group) > 1:\n",
    "                    waiting = group['cum_time'].iloc[-1] - group['cum_time'].iloc[0]\n",
    "                    if waiting >= 0:\n",
    "                        period = classify_period(group[\"hr\"])\n",
    "                        peak_records.append({\n",
    "                            \"from\": stop,\n",
    "                            \"to\": stop,\n",
    "                            \"minutes\": waiting / 60.0,\n",
    "                            \"period\": period\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUMPTIONS:\n",
    "# - BASE_STOPS_ORDER, bus_stations, time_to_seconds, time_to_hr, get_stop, \n",
    "#   classify_period, peak_records (list), and all_files (list) are defined elsewhere.\n",
    "\n",
    "# --- Collect Travel Time Data (from!=to) - CORRECTED: Collect only forward travel (i -> j, where j > i)\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if isinstance(df['time'].iloc[0], str) and df['time'].iloc[0].startswith('('):\n",
    "            df['time'] = df['time'].apply(ast.literal_eval)\n",
    "\n",
    "        # Determine the direction from the filename and set variables accordingly\n",
    "        if \"clockwise\" in file_path:\n",
    "            trip_direction = \"clockwise\"\n",
    "            stations_dict = bus_stations[\"clockwise\"]\n",
    "        else:\n",
    "            trip_direction = \"anit-clockwise\"\n",
    "            stations_dict = bus_stations[\"anti-clockwise\"]\n",
    "            \n",
    "        df['time_sec'] = df['time'].apply(time_to_seconds)\n",
    "        df[\"hr\"]=df[\"time\"].apply(time_to_hr)\n",
    "        df['cum_time'] = df['time_taken'].cumsum().fillna(0)\n",
    "        \n",
    "        stops = []\n",
    "        last_stop = None\n",
    "        for _, row in df.iterrows():\n",
    "            \n",
    "            current_stop = get_stop(row['lat'], row['lon'], bus_stations['anti-clockwise'], last_stop , direction = trip_direction)\n",
    "            stops.append(current_stop)\n",
    "            last_stop = current_stop\n",
    "\n",
    "        df['stop'] = stops\n",
    "\n",
    "        trip_start_sec = df['time_sec'].iloc[0]\n",
    "\n",
    "\n",
    "        df['segment'] = (df['stop'] != df['stop'].shift()).cumsum()\n",
    "        arrivals = df.dropna(subset=[\"stop\"]).groupby(\"segment\").first()\n",
    "        stop_to_cum = dict(zip(arrivals[\"stop\"], zip(arrivals[\"cum_time\"] , arrivals[\"hr\"])))\n",
    "\n",
    "        # ASSUMPTION: BASE_STOPS_ORDER is the Counter-Clockwise order.\n",
    "        route_order = BASE_STOPS_ORDER if trip_direction == \"clockwise\" else list(reversed(BASE_STOPS_ORDER))\n",
    "        visited = [s for s in route_order if s in stop_to_cum]\n",
    "\n",
    "        # Iterate over all unique pairs of visited stops (i, j)\n",
    "        for i in range(len(visited)):\n",
    "            # CRITICAL FIX: Ensure j is always greater than i to record forward movement in time.\n",
    "            for j in range(i + 1, len(visited)): \n",
    "                \n",
    "                from_stop = visited[i]\n",
    "                to_stop = visited[j]\n",
    "\n",
    "                cum_from, hr_from = stop_to_cum[from_stop]\n",
    "                cum_to,   hr_to   = stop_to_cum[to_stop]\n",
    "\n",
    "                minutes = (cum_to - cum_from) / 60.0\n",
    "                \n",
    "                # Safety check: If for any reason minutes are non-positive, skip the record.\n",
    "                if minutes <= 0.0: \n",
    "                    continue \n",
    "\n",
    "                segment_hour = hr_from\n",
    "                period = classify_period(segment_hour)\n",
    "                \n",
    "                # Record only valid, forward-travel segments\n",
    "                peak_records.append({\n",
    "                    \"from\": from_stop, \n",
    "                    \"to\": to_stop, \n",
    "                    \"minutes\": minutes, \n",
    "                    \"period\": period,\n",
    "                    \"direction\": trip_direction\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\" Peak analysis error {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Average Peak Heatmaps + CSV Matrices (Directional Version) ===\n",
    "# ASSUMPTIONS:\n",
    "# - FOLDER_PEAK_HEATMAPS, BASE_STOPS_ORDER, correct_matrix_negatives (function) are defined elsewhere.\n",
    "# - Libraries: import pandas as pd, import numpy as np, import matplotlib.pyplot as plt, import seaborn as sns, import os\n",
    "\n",
    "if peak_records:\n",
    "    df_peak = pd.DataFrame(peak_records)\n",
    "    print(df_peak)\n",
    "    \n",
    "    # BASE_STOPS_ORDER is assumed to be the Counter-Clockwise order.\n",
    "    \n",
    "    def make_directional_peak_heatmap_and_csv(period, direction, title, png_filename, csv_filename):\n",
    "        \n",
    "        # --- FILTERING STEP ---\n",
    "        subset = df_peak[\n",
    "            (df_peak[\"period\"] == period) &\n",
    "            (df_peak[\"direction\"] == direction)\n",
    "        ]\n",
    "\n",
    "        if subset.empty:\n",
    "            print(f\"No data for {period} period in {direction} direction.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n--- Processing {period.capitalize()} - {direction.capitalize()} ---\")\n",
    "        \n",
    "        # 1. Compute stats and cap outliers\n",
    "        stats = subset.groupby(['from', 'to'])['minutes'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        stats['std'] = stats['std'].fillna(0)\n",
    "        \n",
    "        # ACCURACY FIX: Enforce a non-negative lower bound (max(0, mean - 3*std))\n",
    "        stats['lower_bound'] = (stats['mean'] - 3 * stats['std']).clip(lower=0) \n",
    "        stats['upper_bound'] = stats['mean'] + 3 * stats['std']\n",
    "\n",
    "        subset_with_bounds = subset.merge(\n",
    "            stats[['from', 'to', 'lower_bound', 'upper_bound']],\n",
    "            on=['from', 'to'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        subset_clean = subset_with_bounds.copy()\n",
    "        subset_clean['minutes_capped'] = subset_clean['minutes'].clip(\n",
    "            lower=subset_clean['lower_bound'],\n",
    "            upper=subset_clean['upper_bound']\n",
    "        )\n",
    "\n",
    "        print(f\"{period.capitalize()} period, {direction.capitalize()} - Outliers capped (minimum 0 min)\")\n",
    "\n",
    "        # --- Base stop order for the current direction ---\n",
    "        # BASE_STOPS_ORDER is assumed to be the Counter-Clockwise order.\n",
    "        stops_order_for_matrix = BASE_STOPS_ORDER\n",
    "        if direction == \"clockwise\":\n",
    "             # Reverse order for clockwise route to ensure forward travel is displayed logically (top-left to bottom-right)\n",
    "             stops_order_for_matrix = list(reversed(BASE_STOPS_ORDER))\n",
    "        \n",
    "        # 2. Compute average using capped values and reindex\n",
    "        mat = subset_clean.pivot_table(\n",
    "            values='minutes_capped',\n",
    "            index='from',\n",
    "            columns='to',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        mat = mat.reindex(index=stops_order_for_matrix, columns=stops_order_for_matrix).fillna(0.0)\n",
    "\n",
    "        # 3. Apply correction (e.g., setting matrix diagonals/unvisited segments to 0)\n",
    "        mat_corrected = correct_matrix_negatives(mat)\n",
    "\n",
    "        # === Save CSV ===\n",
    "        csv_name = f\"{period}_rush_{direction}_matrix_3sigma_capped.csv\"\n",
    "        csv_path = os.path.join(FOLDER_PEAK_HEATMAPS, csv_name)\n",
    "        mat_corrected.to_csv(csv_path)\n",
    "        print(f\"Saved CSV matrix: {csv_path}\")\n",
    "\n",
    "        # === Plot Heatmap ===\n",
    "        \n",
    "        # 4. Create the Mask based on Direction\n",
    "        \n",
    "        # Start with the existing mask (masking where data is 0.0)\n",
    "        mask_zeros = mat_corrected == 0.0\n",
    "        \n",
    "        # Create a new, stronger directional mask\n",
    "        if direction == \"clockwise\":\n",
    "            # Clockwise (A->B->C) shows forward segments in the Upper Triangle. Mask the STRICT Lower Triangle.\n",
    "            mask_directional = np.tril(np.ones_like(mat_corrected, dtype=bool), k=-1)\n",
    "            \n",
    "        elif direction == \"counter_clockwise\":\n",
    "            # Counter-Clockwise (C->B->A) shows forward segments in the Lower Triangle. Mask the STRICT Upper Triangle.\n",
    "            mask_directional = np.triu(np.ones_like(mat_corrected, dtype=bool), k=1)\n",
    "            \n",
    "        else:\n",
    "             mask_directional = np.zeros_like(mat_corrected, dtype=bool)\n",
    "\n",
    "        # Combine the directional mask with the mask for 0 values\n",
    "        # We only mask the strict upper/lower triangle (k=-1 or k=1), preserving the diagonal (k=0) for wait times.\n",
    "        mask = mask_zeros | mask_directional\n",
    "\n",
    "        # Set min/max values\n",
    "        valid_vals = mat_corrected.values[mat_corrected.values != 0.0]\n",
    "        if valid_vals.size > 0:\n",
    "            min_val = 0.0 # Time must start at 0\n",
    "            max_val = valid_vals.max()\n",
    "        else:\n",
    "            min_val, max_val = 0, 1\n",
    "            \n",
    "        plt.figure(figsize=(15, 12))\n",
    "        sns.heatmap(mat_corrected, annot=True, fmt=\".1f\", cmap=\"RdYlGn_r\", linewidths=0.5, \n",
    "                    mask=mask, # Apply the combined mask here\n",
    "                    cbar_kws={\"label\": \"Avg Time (min)\"}, vmin=min_val, vmax=max_val)\n",
    "        \n",
    "        plt.text(0.5, -0.05,\n",
    "                 f\"Diagonal = Avg Waiting Time | Direction: {direction.capitalize()} | Masked area represents irrelevant reverse segments.\",\n",
    "                 transform=plt.gca().transAxes, fontsize=10, color='blue', ha='center')\n",
    "        \n",
    "        plt.title(f\"{title} - {direction.capitalize()} Route\\n(Relevant segments displayed only)\", fontsize=16)\n",
    "        plt.xlabel(\"To Stop →\")\n",
    "        plt.ylabel(\"From Stop →\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        png_name = f\"{period}_rush_{direction}_heatmap_3sigma_capped.png\"\n",
    "        png_path = os.path.join(FOLDER_PEAK_HEATMAPS, png_name)\n",
    "        plt.savefig(png_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved heatmap: {os.path.basename(png_path)}\")\n",
    "\n",
    "    # === Function Calls ===\n",
    "    # Morning Rush\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"morning\", direction=\"clockwise\",\n",
    "        title=\"Morning Rush (07:00–10:00) Average Journey & Waiting Time\",\n",
    "        png_filename=\"morning_rush_clockwise_heatmap.png\",\n",
    "        csv_filename=\"morning_rush_clockwise_matrix.csv\"\n",
    "    )\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"morning\", direction=\"counter_clockwise\",\n",
    "        title=\"Morning Rush (07:00–10:00) Average Journey & Waiting Time\",\n",
    "        png_filename=\"morning_rush_counter_clockwise_heatmap.png\",\n",
    "        csv_filename=\"morning_rush_counter_clockwise_matrix.csv\"\n",
    "    )\n",
    "\n",
    "    # Evening Rush\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"evening\", direction=\"clockwise\",\n",
    "        title=\"Evening Rush (17:00–19:00) Average Journey & Waiting Time\",\n",
    "        png_filename=\"evening_rush_clockwise_heatmap.png\",\n",
    "        csv_filename=\"evening_rush_clockwise_matrix.csv\"\n",
    "    )\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"evening\", direction=\"counter_clockwise\",\n",
    "        title=\"Evening Rush (17:00–19:00) Average Journey & Waiting Time\",\n",
    "        png_filename=\"evening_rush_counter_clockwise_heatmap.png\",\n",
    "        csv_filename=\"evening_rush_counter_clockwise_matrix.csv\"\n",
    "    )\n",
    "\n",
    "    # Off-Peak Hours\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"free\", direction=\"clockwise\",\n",
    "        title=\"Off-Peak Hours Average Journey & Waiting Time\",\n",
    "        png_filename=\"free_time_clockwise_heatmap.png\",\n",
    "        csv_filename=\"free_time_clockwise_matrix.csv\"\n",
    "    )\n",
    "    make_directional_peak_heatmap_and_csv(\n",
    "        period=\"free\", direction=\"counter_clockwise\",\n",
    "        title=\"Off-Peak Hours Average Journey & Waiting Time\",\n",
    "        png_filename=\"free_time_counter_clockwise_heatmap.png\",\n",
    "        csv_filename=\"free_time_counter_clockwise_matrix.csv\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nAll directional peak heatmaps and CSV matrices successfully generated and saved in 'peak_heatmaps/' folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b38273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Print the values first\n",
    "periods = ['Morning Rush', 'Free Time', 'Evening Rush']\n",
    "\n",
    "\n",
    "# === Create Grouped Bar Chart ===\n",
    "x = np.arange(len(periods))  # positions: 0, 1, 2\n",
    "width = 0.35  # width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bars for Total and Discarded\n",
    "bars1 = ax.bar(x - width/2, total_entries, width, label='Total Entries', color='skyblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, discarded_entries, width, label='Discarded Entries', color='salmon', edgecolor='black')\n",
    "\n",
    "# Add numbers on top of each bar\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{int(height)}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),  # 5 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Time Period', fontsize=12)\n",
    "ax.set_ylabel('Number of Entries', fontsize=12)\n",
    "ax.set_title('Total vs Discarded Entries by Rush Hour Period', fontsize=15, pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(periods, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Tight layout to prevent clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mahanagar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
