{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"lake_updated.csv\")\n",
        "df.columns = [\"from\",\"to\",\"dist\" , \"time\" , \"dir_ref\" , \"rush\"]\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df[['from', 'to' ,\"dist\",  \"dir_ref\" , \"rush\"]]\n",
        "y = df['time']\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1162660c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff56a0e5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34dfcc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test) # If you have a test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',             # We track RMSE\n",
        "    'eta': 0.1,                        # Moderate learning rate (0.05–0.3 works well for overfitting)\n",
        "    'max_depth': 0,                    # 0 = no limit → grow extremely deep trees (key for overfitting!)\n",
        "    'min_child_weight': 1,             # Minimum = 1 → almost no restriction on leaf nodes\n",
        "    'gamma': 0.0,                      # No minimum loss reduction → allow all splits\n",
        "    'subsample': 1.0,                  # Use 100% of rows → no row bagging\n",
        "    'colsample_bytree': 1.0,           # Use 100% of features per tree → no column bagging\n",
        "    'colsample_bylevel': 1.0,          # Use all features at each level\n",
        "    'colsample_bynode': 1.0,           # Use all features at each node\n",
        "    'reg_lambda': 0.0,                 # No L2 regularization\n",
        "    'reg_alpha': 0.0,                  # No L1 regularization\n",
        "    'seed': 27,\n",
        "    'tree_method': 'hist',          # Faster training with deep trees\n",
        "    'device': 'cuda'                # Uncomment if you have GPU (much faster for deep trees)\n",
        "}\n",
        "num_rounds = 100  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.callback import TrainingCallback\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"paper\", font_scale=1.3)\n",
        "\n",
        "print(\"Starting XGBoost regression training...\")\n",
        "\n",
        "# ==================== REGRESSION PARAMETERS (NO scale_pos_weight!) ====================\n",
        "\n",
        "\n",
        "# ==================== SAFE & ROBUST CALLBACK ====================\n",
        "class LoggingCallback(TrainingCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "        self.epoch_times = []\n",
        "        self.epoch_start = time.time()\n",
        "\n",
        "    def after_iteration(self, model, epoch, evals_log):\n",
        "        # Measure time for this iteration\n",
        "        epoch_time = time.time() - self.epoch_start\n",
        "        self.epoch_times.append(epoch_time)\n",
        "\n",
        "        # Safety: skip if no eval log yet (rare but possible on iter 0)\n",
        "        if not evals_log:\n",
        "            print(f\"[{epoch+1:4d}] No metrics yet | time: {epoch_time:.2f}s\")\n",
        "            self.epoch_start = time.time()\n",
        "            return False\n",
        "\n",
        "        # Get the metric name safely (should be 'rmse')\n",
        "        metric_name = next(iter(evals_log['train']))  # First and only key\n",
        "\n",
        "        # Get values safely\n",
        "        train_val = evals_log['train'][metric_name][-1]\n",
        "        test_val = evals_log['test'][metric_name][-1] if 'test' in evals_log else None\n",
        "\n",
        "        # Print progress\n",
        "        print_str = f\"[{epoch+1:4d}] train-{metric_name}: {train_val:.6f} | time: {epoch_time:.2f}s\"\n",
        "        if test_val is not None:\n",
        "            print_str += f\" | test-{metric_name}: {test_val:.6f}\"\n",
        "\n",
        "        # Record metrics\n",
        "        self.metrics.append({\n",
        "            'iteration': epoch + 1,\n",
        "            'dataset': 'train',\n",
        "            'metric': metric_name,\n",
        "            'value': train_val\n",
        "        })\n",
        "        if test_val is not None:\n",
        "            self.metrics.append({\n",
        "                'iteration': epoch + 1,\n",
        "                'dataset': 'test',\n",
        "                'metric': metric_name,\n",
        "                'value': test_val\n",
        "            })\n",
        "\n",
        "        # Reset timer for next iteration\n",
        "        self.epoch_start = time.time()\n",
        "        return False\n",
        "\n",
        "# ==================== TRAINING WITH CALLBACK ====================\n",
        "log_callback = LoggingCallback()\n",
        "\n",
        "start_total = time.time()\n",
        "\n",
        "model = xgb.train(\n",
        "    params=params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=num_rounds,\n",
        "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
        "    callbacks=[log_callback],       # ← NOW ACTUALLY USED\n",
        "    verbose_eval=False              # ← Let callback handle printing\n",
        ")\n",
        "model.save_model(\"model_eval/xgb_model.pkl\")\n",
        "total_time = time.time() - start_total\n",
        "print(f\"\\nTraining finished in {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
        "\n",
        "# ==================== SAFE PLOTTING ====================\n",
        "if not log_callback.metrics:\n",
        "    print(\"ERROR: No metrics were collected. Training may have failed.\")\n",
        "else:\n",
        "    metrics_df = pd.DataFrame(log_callback.metrics)\n",
        "    \n",
        "    # Simple and safe pivot: separate columns for train_rmse and test_rmse\n",
        "    plot_df = metrics_df.pivot(index='iteration', columns='dataset', values='value').reset_index()\n",
        "    plot_df.columns = ['iteration', 'train_rmse', 'test_rmse']  # Clear column names\n",
        "    \n",
        "    # Cumulative time (aligned to end of each iteration)\n",
        "    plot_df['cumulative_time'] = pd.Series(log_callback.epoch_times).cumsum()\n",
        "\n",
        "    # Plot 1: RMSE over iterations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=plot_df, x='iteration', y='train_rmse', label='Training RMSE', linewidth=2.5)\n",
        "    if 'test_rmse' in plot_df.columns and not plot_df['test_rmse'].isna().all():\n",
        "        sns.lineplot(data=plot_df, x='iteration', y='test_rmse', label='Validation RMSE', linewidth=2.5)\n",
        "    plt.title('XGBoost Regression: RMSE over Iterations', fontsize=16)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: RMSE vs Time\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=plot_df, x='cumulative_time', y='train_rmse', label='Training RMSE')\n",
        "    if 'test_rmse' in plot_df.columns and not plot_df['test_rmse'].isna().all():\n",
        "        sns.lineplot(data=plot_df, x='cumulative_time', y='test_rmse', label='Validation RMSE')\n",
        "    \n",
        "    plt.title('RMSE vs Training Time')\n",
        "    plt.xlabel('Cumulative Time (seconds)')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have:\n",
        "predictions = model.predict(dtest)\n",
        "\n",
        "# y_test = actual test targets (as pandas Series or numpy array)\n",
        "\n",
        "# Convert to numpy for easier indexing\n",
        "actual = y_test.values if hasattr(y_test, 'values') else y_test\n",
        "pred = predictions\n",
        "\n",
        "# Use first N samples for clear visualization (e.g., 50–100 to avoid clutter)\n",
        "N = 80\n",
        "indices = np.arange(N)\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Width of bars\n",
        "width = 0.35\n",
        "\n",
        "# Side-by-side bar plot: Actual vs Predicted\n",
        "plt.bar(indices - width/2, actual[:N], width, label='Actual Time', color='skyblue', alpha=0.8)\n",
        "plt.bar(indices + width/2, pred[:N], width, label='Predicted Time', color='salmon', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Sample Index (Test Set)', fontsize=12)\n",
        "plt.ylabel('Travel Time', fontsize=12)\n",
        "plt.title('Actual vs Predicted Travel Time (First 80 Test Samples)', fontsize=16)\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Optional: Add a line connecting actual and predicted for each point (helps see errors)\n",
        "for i in range(N):\n",
        "    plt.plot([indices[i] - width/2, indices[i] + width/2], [actual[i], pred[i]], \n",
        "             color='gray', linewidth=0.8, alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc0b77d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data\n",
        "actual = y_test.values if hasattr(y_test, 'values') else y_test\n",
        "pred = predictions\n",
        "\n",
        "# Calculate metrics\n",
        "r2 = r2_score(actual, pred)\n",
        "rmse = np.sqrt(mean_squared_error(actual, pred))\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Seaborn scatter plot\n",
        "sns.scatterplot(x=actual, y=pred, alpha=0.6, color='teal', s=60)  # s controls point size\n",
        "\n",
        "# Add the perfect prediction line (y = x)\n",
        "min_val = min(actual.min(), pred.min())\n",
        "max_val = max(actual.max(), pred.max())\n",
        "\n",
        "# Use Seaborn's lineplot for the diagonal reference line\n",
        "sns.lineplot(x=[min_val, max_val], y=[min_val, max_val], \n",
        "             color='red', linestyle='--', linewidth=2, label='Perfect Prediction (y=x)')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('Actual Travel Time (minutes)', fontsize=12)\n",
        "plt.ylabel('Predicted Travel Time (minutes)', fontsize=12)\n",
        "plt.title(f'Predicted vs Actual Travel Time\\n(R² = {r2:.3f} | RMSE = {rmse:.2f})', \n",
        "          fontsize=14, pad=20)\n",
        "\n",
        "# Legend (Seaborn places it automatically, but we can tweak if needed)\n",
        "plt.legend(fontsize=11)\n",
        "\n",
        "# Final adjustments\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"\\nModel Evaluation on Test Set:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R2): {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_model(\"./model_eval/xgb_model_final.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
